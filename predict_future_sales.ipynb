{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cmannnn/predict_future_sales/blob/main/predict_future_sales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4SoFslGfRpJ"
   },
   "source": [
    "# Predict Future Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJaaDvndfbOO"
   },
   "source": [
    "You are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GwxSxIkblYe"
   },
   "source": [
    "# File descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpAc3j6Eb2hI"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pj9IqueibjQL"
   },
   "source": [
    "itemcats_df - supplemental information about the items categories.\n",
    "\n",
    "items_df - supplemental information about the items/products.\n",
    "\n",
    "salestrain_df - the training set. Daily historical data from January 2013 to October 2015.\n",
    "\n",
    "shops_df - supplemental information about the shops.\n",
    "\n",
    "test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n",
    "\n",
    "sample_submission.csv - a sample submission file in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-UxcjnUffCN"
   },
   "source": [
    "# Imports and uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTSLNlmdd5lX"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "#import specific sklearn packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#encoders, tested several of them\n",
    "from sklearn.preprocessing import OrdinalEncoder # remove later, don't use this\n",
    "from sklearn.preprocessing import LabelEncoder # remove later, don't use this\n",
    "from sklearn.preprocessing import OneHotEncoder # probably don't use this, instead use pd.get_dummies\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFj3ad_MNaoz"
   },
   "outputs": [],
   "source": [
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wezZf71jNjPJ"
   },
   "outputs": [],
   "source": [
    "# UPLOAD YOUR kaggle.json KEY HERE\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5MjpqkNOGj8"
   },
   "outputs": [],
   "source": [
    "!kaggle competitions list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDUtQqcJYNLt"
   },
   "outputs": [],
   "source": [
    "!mkdir -p predict_future_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wgoyloq3NmIX"
   },
   "outputs": [],
   "source": [
    "# UPLOAD 6 FILES HERE\n",
    "uploaded2 = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o_BHflNj8B0"
   },
   "outputs": [],
   "source": [
    "!cp test.csv predict_future_sales/\n",
    "!cp item_categories.csv predict_future_sales/\n",
    "!cp items.csv predict_future_sales/\n",
    "!cp sales_train.csv predict_future_sales/\n",
    "!cp sample_submission.csv predict_future_sales/\n",
    "!cp shops.csv predict_future_sales/\n",
    "!cp test.csv predict_future_sales/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mY4s3UubFhz"
   },
   "source": [
    "# Importing All Predict Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwqnebRuocPf"
   },
   "outputs": [],
   "source": [
    "#importing all the data frames\n",
    "itemcats_df = pd.read_csv(\"predict_future_sales/item_categories.csv\") #Item Categories\n",
    "items_df = pd.read_csv(\"predict_future_sales/items.csv\") # Items\n",
    "salestrain_df = pd.read_csv(\"predict_future_sales/sales_train.csv\") #Sales Data - Training\n",
    "shops_df = pd.read_csv(\"predict_future_sales/shops.csv\") # Shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkKM_5O-LcE5"
   },
   "outputs": [],
   "source": [
    "# sample submission dataframe\n",
    "sample_df = pd.read_csv('predict_future_sales/sample_submission.csv')\n",
    "\n",
    "# test data frame\n",
    "test_df = pd.read_csv('predict_future_sales/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JVZ-b2LbqjR"
   },
   "source": [
    "# Data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lg8_zM9rbRsN"
   },
   "outputs": [],
   "source": [
    "# creating a master list of all dataframes\n",
    "df_list = [itemcats_df, items_df, salestrain_df, shops_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFrFewFkbvdB"
   },
   "outputs": [],
   "source": [
    "# creating a loop to print head, description, info, and null sum for each datafram in master list \n",
    "for df in df_list:\n",
    "    print(f'DATAFRAME HEAD: {df.head()}')\n",
    "    print('-----------------------')\n",
    "    print(f'DATAFRAME DESCRIPTION: {df.describe()}')\n",
    "    print('-----------------------')\n",
    "    print(f'DATAFAME INFO: {df.info()}')\n",
    "    print('-----------------------')\n",
    "    print(f'DATAFRAME NULL SUM: {df.isnull().sum()}')\n",
    "    print('------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    for col in df:\n",
    "        print(col + \" has the following number of unique values:\")\n",
    "        print(df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAaoFHoIcBDg"
   },
   "outputs": [],
   "source": [
    "# checking itemcats_df column data types\n",
    "itemcats_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-iFIwgBcC66"
   },
   "outputs": [],
   "source": [
    "# checking item_df column data types\n",
    "items_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtdzLWs9cF9B"
   },
   "outputs": [],
   "source": [
    "# checking salestrain_df data types\n",
    "salestrain_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZfrZjc7cMde"
   },
   "outputs": [],
   "source": [
    "# changing salestrain 'item_cnt_day' col from float64 -> int16\n",
    "salestrain_df['item_cnt_day'] = pd.to_numeric(salestrain_df['item_cnt_day'], downcast='signed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8_43wo0cole"
   },
   "outputs": [],
   "source": [
    "# changing salestrain 'date' col from object -> datetime\n",
    "salestrain_df['date'] = pd.to_datetime(salestrain_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxdsCpTYcrRd"
   },
   "outputs": [],
   "source": [
    "shops_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnjZjLxSL-jl"
   },
   "outputs": [],
   "source": [
    "# the df info showed some items with negative price, check them here\n",
    "salestrain_df[salestrain_df['item_price'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGz0Jt1jL-jl"
   },
   "outputs": [],
   "source": [
    "# this item is a game, released in 2013, so very unlikely that price is negative: change price to median price of this item in the same date_block_num\n",
    "salestrain_df.loc[salestrain_df['item_price'] < 0, 'item_price'] = salestrain_df[(salestrain_df['item_price'] > 0) & (salestrain_df['date_block_num'] == 4) & \n",
    "                                                                                                           (salestrain_df['item_id'] == 2973)]['item_price'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHVIBl7Xcv4E"
   },
   "source": [
    "# Combine DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjnsHvz1cuOO"
   },
   "outputs": [],
   "source": [
    "# combining training dataframes into master training set\n",
    "salestrain_df_combined = salestrain_df.merge(items_df,on='item_id')\n",
    "salestrain_df_combined = salestrain_df_combined.merge(itemcats_df,on='item_category_id')\n",
    "salestrain_df_combined = salestrain_df_combined.merge(shops_df,on='shop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jtr7A6KXc0m0"
   },
   "outputs": [],
   "source": [
    "salestrain_df_combined.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syHw5OsIc4h6"
   },
   "outputs": [],
   "source": [
    "# setting the id col, and 2 target cols based on testing data\n",
    "id_col, target_col_item, target_col_shop = salestrain_df.index, salestrain_df_combined['item_id'], salestrain_df_combined['shop_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkxytbOZL-jn"
   },
   "source": [
    "# Outliers\n",
    "Based on the information from salestrain_df, the max price of some items is insanely high. In addition, quantity of items sold on one day is extremely high for some items. These particular rows are investigated in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nN6cFzA5L-jn"
   },
   "outputs": [],
   "source": [
    "# checking outliers for quantity of items sold per day\n",
    "plt.figure(figsize = (20, 10))\n",
    "sns.boxplot(x=salestrain_df_combined['item_cnt_day'])\n",
    "plt.ylabel('Number of items sold per day')\n",
    "plt.title('Boxplot for checking outliers for number of items sold per day')\n",
    "\n",
    "# checking outliers for item prices\n",
    "plt.figure(figsize = (20, 10))\n",
    "sns.boxplot(x=salestrain_df_combined['item_price'])\n",
    "plt.ylabel('Number of items sold per day')\n",
    "plt.title('Boxplot for checking outliers for number of items sold per day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ir5HFlwYL-jn"
   },
   "outputs": [],
   "source": [
    "# boxplot shows quantity for 1 particular sale is very high. Check all sales where quantity is bigger than 500\n",
    "# can imagine that one shop has 2169 deliveries at some peak days or some companies buying 1000 t-shirts\n",
    "salestrain_df_combined[salestrain_df_combined['item_cnt_day'] > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihpK7KyiL-jn"
   },
   "outputs": [],
   "source": [
    "# boxplot shows price for one item is very high, check all items above 40000\n",
    "# most expensive item is a license for some software package (VPN remote working) for 522 users (roughly 4000 US dollars)\n",
    "salestrain_df_combined[salestrain_df_combined['item_price'] > 40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoP__0VrL-jo"
   },
   "outputs": [],
   "source": [
    "# for now, remove the row with the license for software package and any sale with quantity more or equal to 1000, discuss this next meeting\n",
    "salestrain_df_combined = salestrain_df_combined[(salestrain_df_combined['item_cnt_day'] < 1000) & (salestrain_df_combined['item_price'] < 300000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOxMQcL9dMuu"
   },
   "source": [
    "# Data Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LH4oZ8mdLNt"
   },
   "outputs": [],
   "source": [
    "# adding features for separating date\n",
    "salestrain_df_combined['year'] = salestrain_df_combined.date.dt.year\n",
    "salestrain_df_combined['month'] = salestrain_df_combined.date.dt.month\n",
    "salestrain_df_combined['day'] = salestrain_df_combined.date.dt.day\n",
    "\n",
    "# adding revenue feature\n",
    "salestrain_df_combined['revenue'] = salestrain_df_combined.item_price * salestrain_df_combined.item_cnt_day\n",
    "salestrain_df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbiVOlOcdnMl"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "salestrain_df_combined.groupby('date_block_num').date.count().plot.line(title='Sales by Month', color='green', figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84XkX343f9Ru"
   },
   "source": [
    "As we can see, sales are decreasing through the years. \n",
    "We also see a spike sale during december. \n",
    "Since, the dataset is up to October, are we seeing some spike in year 2015?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pj-psTcbjH26"
   },
   "outputs": [],
   "source": [
    "# time series scatterplot of revenue highlighting years\n",
    "plt.figure(figsize = (20, 10))\n",
    "sns.scatterplot(x = salestrain_df_combined['date'], y = salestrain_df_combined['revenue'], hue = salestrain_df_combined['year'], palette='deep')\n",
    "plt.ylim(salestrain_df_combined['revenue'].min(), 1900000)\n",
    "#plt.arrow(2013-11-29, 1829990.0000013, 2013-11-30, 1700000, head_width = 10, head_length = 11)\n",
    "#plt.axvline()\n",
    "plt.ylabel('Revenue (millions of rubles)')\n",
    "plt.legend(fontsize='x-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0sQa1EvZygz"
   },
   "outputs": [],
   "source": [
    "# printing most amount of revenue and least amount of revenue\n",
    "print(f'The least amount of revenue recorded is: {salestrain_df_combined.revenue.min()}')\n",
    "print(f'The most amount of revenue recorded is: {salestrain_df_combined.revenue.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mce2e8MlUejX"
   },
   "outputs": [],
   "source": [
    "# day with the highest amount of revenue\n",
    "print('The date with the most amount of revenue is:', salestrain_df_combined.loc[salestrain_df_combined['revenue'] == 734571.99999936, 'date'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CwWaJYUVfue"
   },
   "outputs": [],
   "source": [
    "# masking revenue numbers to show negative revenue\n",
    "# not sure how to have negative revenue\n",
    "# item returns?\n",
    "salestrain_neg_mask = salestrain_df_combined['revenue'] < 0\n",
    "salestrain_df_combined[salestrain_neg_mask].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uJEbpZSwvTw"
   },
   "outputs": [],
   "source": [
    "# grouping revenue by month and rounding number\n",
    "monthly_group = salestrain_df_combined.groupby(by=['month'])\n",
    "monthly_group_sum = monthly_group['revenue'].sum().round()      \n",
    "monthly_group_sum                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yc0KkeSf2Gvh"
   },
   "outputs": [],
   "source": [
    "# normalizing data for visualization\n",
    "monthly_group_norm = (monthly_group_sum - monthly_group_sum.mean()) / (monthly_group_sum.max() - monthly_group_sum.min())\n",
    "monthly_group_norm = abs(monthly_group_norm)\n",
    "monthly_group_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKV7y25_7nVN"
   },
   "outputs": [],
   "source": [
    "# graphing normalized revenue\n",
    "# look at those december sales!!\n",
    "ax, fig = plt.subplots(figsize=(20,10))\n",
    "cols = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.bar(cols, monthly_group_norm, color='red')\n",
    "plt.title('Normalized revenue by month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhLWOd8peBDd"
   },
   "outputs": [],
   "source": [
    "# Plot total revenue by month for each year\n",
    "salestrain_df_combined.groupby(['month', 'year']).sum()['revenue'].unstack().plot(figsize=(20, 10))\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTYRFbtfgHzE"
   },
   "source": [
    "There are no sales spike on december 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIGvE9QA6Q-f"
   },
   "outputs": [],
   "source": [
    "# creating list of top selling items\n",
    "top_items = [20949, 5822, 17717, 2808, 4181, 7856, 3732, 2308, 4870, 3734]\n",
    "\n",
    "# creating empty list to append to\n",
    "top_item_cols = []\n",
    "\n",
    "# loop to append item name based on top selling items\n",
    "for items in top_items:\n",
    "  top_item_cols.append(salestrain_df_combined.loc[salestrain_df_combined['item_id'] == items, 'item_name'].values[0])\n",
    "\n",
    "top_item_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yf6ZXrBQeLa9"
   },
   "outputs": [],
   "source": [
    "# Plot the top 10 most items sold\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.countplot(y = 'item_id', hue = 'year', data = salestrain_df_combined, order = salestrain_df_combined['item_id'].value_counts().iloc[:10].index)\n",
    "ax.set_yticklabels(top_item_cols)\n",
    "plt.xlim(0, 20000)\n",
    "plt.xlabel('Sales of Sold Items')\n",
    "plt.ylabel('Item IDs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHw_JHDEDrkq"
   },
   "outputs": [],
   "source": [
    "salestrain_df_combined['item_id'] == 20949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSjQtW30gLb5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "sns.heatmap(salestrain_df_combined.corr(), cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmaqVg7Bi9tG"
   },
   "outputs": [],
   "source": [
    "# plotting histogram of the price of items\n",
    "# some high ticket items, lots \n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.histplot(x='item_price', data=salestrain_df)\n",
    "plt.xlim(0, 4000)\n",
    "plt.title('Item price histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7AkQvgxBfXn"
   },
   "source": [
    "create item price bands to graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqSMaSwVBSdM"
   },
   "source": [
    "whisker plot with yearly revenue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlU1y_1fIzMX"
   },
   "source": [
    "pair plot variation? diag_kind='hist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYZBAY-bMJcF"
   },
   "source": [
    "sns.scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icppcJg8DfzH"
   },
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vitZ_85DxmV"
   },
   "source": [
    "Make features for MA: 7, 30, 60, 90 days, season, month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ1Wez84D6Uj"
   },
   "source": [
    "Convert into categorical - Store, Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7Lubs0qDlR4"
   },
   "outputs": [],
   "source": [
    "# item names with their revenue and sold count\n",
    "item_names = salestrain_df_combined.groupby('item_name').agg({'revenue':'sum', 'item_cnt_day':'count'}).reset_index().rename(columns={'item_name': 'Item Name', 'revenue': 'Revenue', 'item_cnt_day': 'Number of Sales'})\n",
    "item_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_YiIJRtEJcc"
   },
   "outputs": [],
   "source": [
    "# top 10 items based on their revenue\n",
    "top10_revenue = item_names.nlargest(10, 'Revenue')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.barplot(data=top10_revenue, hue='Number of Sales', x='Revenue', y='Item Name')\n",
    "plt.title(\"Top 10 Items based on Revenue including their Number of Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvbCUMqTEaFb"
   },
   "outputs": [],
   "source": [
    "# top 10 items based on their revenue\n",
    "top10_sales = item_names.nlargest(10, 'Number of Sales')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.barplot(data=top10_sales, x='Number of Sales', y='Item Name', hue='Revenue')\n",
    "plt.title(\"Top 10 Items based on Sales including their Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFKnw4c8E0Pw"
   },
   "outputs": [],
   "source": [
    "# category names and how many items in it.\n",
    "category_names = salestrain_df_combined.groupby(['item_category_name']).agg({'item_name':'count'}).reset_index().rename(columns={'item_category_name': 'Category Name','item_name': 'Number of Items'})\n",
    "category_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vY8bs31bFH7Z"
   },
   "outputs": [],
   "source": [
    "#Find total sales by item\n",
    "total_sales_by_item = salestrain_df_combined.groupby('item_id')['item_cnt_day'].sum().reset_index()\n",
    "\n",
    "#Sort descending\n",
    "total_sales_by_item.sort_values(by='item_cnt_day',ascending=False,inplace=True)\n",
    "total_sales_by_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-dOSVpdFWUt"
   },
   "outputs": [],
   "source": [
    "#Merge together so we can get item_name with total sales values\n",
    "total_sales_by_item_with_name = total_sales_by_item.merge(items_df[['item_name','item_id']],on='item_id')\n",
    "total_sales_by_item_with_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKxEl2U-GWQt"
   },
   "outputs": [],
   "source": [
    "total_sales_by_item['item_cnt_day'].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIcGTIpVFuba"
   },
   "outputs": [],
   "source": [
    "# updated scatterplot showing top 20 best selling items day to day\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.scatter(y = total_sales_by_item['item_cnt_day'].iloc[:20], x = total_sales_by_item['item_id'].iloc[:20])\n",
    "plt.xlabel('Item ID')\n",
    "plt.ylabel('Item count per day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MchAi0wABchP"
   },
   "source": [
    "## Recommendation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiT7a3a6Fwnz"
   },
   "outputs": [],
   "source": [
    "# Get sales by item, by date - again, not very exciting,so I also used fillna()\n",
    "sales_totals_by_day = salestrain_df_combined.pivot_table(index='item_id',columns=['year', 'month', 'day'],values='item_cnt_day').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDvgXPlkF80W"
   },
   "outputs": [],
   "source": [
    "# Clean up the multi-level index\n",
    "sales_totals_by_day.columns = sales_totals_by_day.columns.droplevel().droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAQ_Imf3GEaq"
   },
   "outputs": [],
   "source": [
    "sales_totals_by_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z37oDt15GHZ-"
   },
   "outputs": [],
   "source": [
    "# Shape of the resulting DF - items x days\n",
    "sales_totals_by_day.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfZEnry-GMEt"
   },
   "outputs": [],
   "source": [
    "#Transpose this\n",
    "sales_totals_by_day_transposed = sales_totals_by_day.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YhaC0WUGRlS"
   },
   "outputs": [],
   "source": [
    "sales_totals_by_day_transposed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYr1HT2nGXOO"
   },
   "outputs": [],
   "source": [
    "# Recommendation engine - find the closest matches to an item in terms of daily sales\n",
    "# Similar to finding similar movies based on movie rating\n",
    "\n",
    "#Select a popular item\n",
    "selected_item = 2808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp9PZsfWGd6c"
   },
   "outputs": [],
   "source": [
    "#Pull out the column of that day's sales\n",
    "selected_item_sales = sales_totals_by_day_transposed[selected_item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG2LdGkoGhsE"
   },
   "outputs": [],
   "source": [
    "#Run correlation with every other column to find similar sales patterns\n",
    "similarItems = sales_totals_by_day_transposed.corrwith(selected_item_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK2VKoiAGk9f"
   },
   "outputs": [],
   "source": [
    "#Convert from a series to a DF\n",
    "similarItems_df = pd.DataFrame(similarItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRHV_bUZGpID"
   },
   "outputs": [],
   "source": [
    "similarItems_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnx0ETi9Gsg5"
   },
   "outputs": [],
   "source": [
    "#Rename columns\n",
    "similarItems_df.columns=['similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgPgvfqFGvcE"
   },
   "outputs": [],
   "source": [
    "similarItems_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKcstmtOGyel"
   },
   "outputs": [],
   "source": [
    "#Merge back item names\n",
    "similarItems_df=similarItems_df.merge(items_df[['item_name','item_id']],left_index=True,right_on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsvnieMlG2bR"
   },
   "outputs": [],
   "source": [
    "#Sort\n",
    "similarItems_df_sorted=similarItems_df.sort_values(by='similarity',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HM_oJ7AG5u-"
   },
   "outputs": [],
   "source": [
    "#The top results for item 2808 seem to make some sense (games), but they also include programming books and other things-inconclusive?\n",
    "similarItems_df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6lqZ6_wG9gP"
   },
   "outputs": [],
   "source": [
    "#Function to return the values:\n",
    "\n",
    "def find_similar_based_on_daily_sales(selected_item,sales_totals_by_day_transposed=sales_totals_by_day_transposed):\n",
    "    selected_item_sales=sales_totals_by_day_transposed[selected_item]\n",
    "    similarItems = sales_totals_by_day_transposed.corrwith(selected_item_sales)\n",
    "    similarItems_df=pd.DataFrame(similarItems)\n",
    "    similarItems_df.columns=['similarity']\n",
    "    similarItems_df=similarItems_df.merge(items_df[['item_name','item_id']],left_index=True,right_on='item_id')\n",
    "    similarItems_df_sorted=similarItems_df.sort_values(by='similarity',ascending=False)\n",
    "    print(similarItems_df_sorted.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_lBhDouHBSe"
   },
   "outputs": [],
   "source": [
    "find_similar_based_on_daily_sales(2808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFqJ3ofRHEze"
   },
   "outputs": [],
   "source": [
    "#What is similar to those corporate T-shirts?\n",
    "find_similar_based_on_daily_sales(20949)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnVK8ybLHMNA"
   },
   "outputs": [],
   "source": [
    "#What is similar to minecraft?\n",
    "find_similar_based_on_daily_sales(4870)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVhvqxxgHUKd"
   },
   "outputs": [],
   "source": [
    "#This item averaged 5 sales per day, max of 35, so it wasn't distorted by one overwhelmingly high corporate order\n",
    "sales_totals_by_day_transposed[20949].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKVDe7cmG5wH"
   },
   "source": [
    "## Rolling window K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1PZjrYqLL3H"
   },
   "outputs": [],
   "source": [
    "salestrain_df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI3YIdocOH1e"
   },
   "outputs": [],
   "source": [
    "# setting X as date col and y as revenue col to be split\n",
    "X = salestrain_df_combined['date']\n",
    "y = salestrain_df_combined['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CiMrixpKjhM"
   },
   "outputs": [],
   "source": [
    "# creating the time series split, one split for every month in the year\n",
    "tscv = TimeSeriesSplit(n_splits = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXSW46VaL7g8"
   },
   "outputs": [],
   "source": [
    "# looping through every time series split \n",
    "for train_index, test_index in tscv.split(X):\n",
    "  print(f'TRAIN SHAPE: {train_index.shape} TEST SHAPE: {test_index.shape}')\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[train_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6TAj7fUryKg"
   },
   "source": [
    "## Removing features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4ivXmd-2x5T"
   },
   "outputs": [],
   "source": [
    "salestrain_df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsjTfbrl2UIe"
   },
   "outputs": [],
   "source": [
    "train_set = salestrain_df_combined.drop(columns = ['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7tGpOca4V3a"
   },
   "outputs": [],
   "source": [
    "# mapping date col to ordinal for use with VarianceThreshold\n",
    "train_set['date'] = train_set['date'].map(dt.datetime.toordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmXty02xZTXk"
   },
   "outputs": [],
   "source": [
    "for col in ['item_name', 'item_category_name', 'shop_name']:\n",
    "   train_set[col] = LabelEncoder().fit_transform(train_set[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCA_TBnX28L-"
   },
   "outputs": [],
   "source": [
    "test_set = salestrain_df_combined['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnKtJvYF3Ick"
   },
   "outputs": [],
   "source": [
    "train_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcQ81fFb3IpB"
   },
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AzdTXaCsUVS"
   },
   "outputs": [],
   "source": [
    "# creating the Variance Threshold and setting variance to 10 -> can mess around with this number\n",
    "threshold = VarianceThreshold(threshold = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiSFnJqHsfKE"
   },
   "outputs": [],
   "source": [
    "# fit transforming variance threshold to train set\n",
    "high_variance = threshold.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrjzkTlTlJu8"
   },
   "outputs": [],
   "source": [
    "# seeing which cols do not have high variance\n",
    "train_set.columns[high_variance.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXytUrgIlIbO"
   },
   "outputs": [],
   "source": [
    "# seeing which cols have low variance\n",
    "constant_cols = [column for column in train_set.columns if column not in train_set.columns[high_variance.get_support()]]\n",
    "print(f'The columns with low(ish) variance is: {constant_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgETveekchhL"
   },
   "source": [
    "All of our features have high variance, therefore they don't have to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_by84Y8SBuKb"
   },
   "source": [
    "## K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARJFxi8_CWGO"
   },
   "outputs": [],
   "source": [
    "#salestrain_df.head()\n",
    "salestrain_df = salestrain_df.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awK01tfoCYJg"
   },
   "outputs": [],
   "source": [
    "salestrain_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u3UQVVZCbAP"
   },
   "outputs": [],
   "source": [
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "\n",
    "# Fit the pipeline to samples\n",
    "pipeline.fit(np.array(salestrain_df))\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(salestrain_df)\n",
    "\n",
    "# Create a DataFrame with labels and prices as columns: salestrain_df\n",
    "#salestrain_df = pd.DataFrame({'item_price': item_price, 'item_cnt_day': item_cnt_day})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(salestrain_df['item_price'], salestrain_df['item_cnt_day'])\n",
    "\n",
    "# Display ct\n",
    "print(ct.sort_values('item_price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zewAZXosCzvK"
   },
   "outputs": [],
   "source": [
    "# plotting clusters\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(ct)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icKqBH3IBzXZ"
   },
   "outputs": [],
   "source": [
    "# reshaping values to workable vector\n",
    "dfx = salestrain_df.values.reshape(1, -1)\n",
    "dfy = salestrain_df.values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFHmuApiCDoK"
   },
   "outputs": [],
   "source": [
    "# WHY DELETING?\n",
    "\n",
    "# delete part 1\n",
    "#dfx = np.delete(dfx, 0)\n",
    "#dfy = np.delete(dfy, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe_8QVcoCDvA"
   },
   "outputs": [],
   "source": [
    "# WHY DELETING?\n",
    "\n",
    "# delete part 2\n",
    "#dfx = np.delete(dfx, 1)\n",
    "#dfy = np.delete(dfy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-h2T_fcC3h7"
   },
   "outputs": [],
   "source": [
    "print(f'dfx shape: {dfx.shape}')\n",
    "print(f'dfy shape: {dfy.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8ht_Z5PC7lL"
   },
   "outputs": [],
   "source": [
    "# setting variables train test split\n",
    "X = dfx\n",
    "y = dfy\n",
    "\n",
    "# running logistic regression baseline\n",
    "logreg =  LogisticRegression()\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLPLECshxVxN"
   },
   "outputs": [],
   "source": [
    "# checking train, test shapes\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKNqiZsDw9np"
   },
   "outputs": [],
   "source": [
    "#\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred =logreg.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Yaxr4kiDGys"
   },
   "outputs": [],
   "source": [
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LErutUyxRu51"
   },
   "source": [
    "## Categorical features\n",
    "Created different datasets with OHE encoding, mean/target encoding, frequency encoding and hash encoding\n",
    "\n",
    "#### OHE\n",
    "OHE does not work, memory issue due to the fact that there are 22150 unique item names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e7vSuc3R09Q"
   },
   "outputs": [],
   "source": [
    "# item_name unique 22150 values --> colossal number of columns, not the best option since shape will be (2935846, 21787)\n",
    "cols_to_encode = ['item_category_name', 'shop_name']\n",
    "salestrain_df_ohe = salestrain_df_combined.copy()\n",
    "\n",
    "# change float64 to float16 and int64 to int16 to reduce memory usage\n",
    "salestrain_df_ohe['item_price'] = salestrain_df_ohe['item_price'].astype(np.float16)\n",
    "cols_int = salestrain_df_ohe.select_dtypes('int64').columns\n",
    "for col in cols_int:\n",
    "    salestrain_df_ohe[col] = salestrain_df_ohe[col].astype(np.int16)\n",
    "\n",
    "# OHE\n",
    "salestrain_df_ohe = pd.get_dummies(salestrain_df_ohe, prefix=['item_category_name', 'shop_name'], columns=['item_category_name', 'shop_name'], dtype=np.int16)\n",
    "\n",
    "# renaming\n",
    "item_cat_iter = 0\n",
    "shop_iter = 0\n",
    "for col in salestrain_df_ohe.columns:\n",
    "    if col.startswith('item_category_name_'):\n",
    "        salestrain_df_ohe.rename(columns={col: \"item_category_name_\" + str(item_cat_iter)}, inplace=True)\n",
    "        item_cat_iter += 1\n",
    "    if col.startswith('shop_name_'):\n",
    "        salestrain_df_ohe.rename(columns={col: \"shop_name_\" + str(shop_iter)}, inplace=True)\n",
    "        shop_iter += 1\n",
    "\n",
    "# sounds good doesn't work: too many different categories for item_name\n",
    "# cols_to_encode = ['item_name', 'item_category_name', 'shop_name']\n",
    "# for col in cols_to_encode:\n",
    "#     salestrain_df_ohe = pd.get_dummies(salestrain_df_ohe, prefix=[col], columns=[col])\n",
    "#     ohe_enc = OneHotEncoder()\n",
    "#     ohe_enc = ohe_enc.fit_transform(salestrain_df_ohe[[col]]).astype(np.int16).toarray()\n",
    "#     encoded_cols = pd.DataFrame(ohe_enc)\n",
    "#     salestrain_df_ohe = pd.concat([salestrain_df_ohe, encoded_cols], axis = 1)\n",
    "    \n",
    "salestrain_df_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency encoder\n",
    "Set categorical features to frequency they occur in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ['item_name', 'item_category_name', 'shop_name']\n",
    "salestrain_df_freq = salestrain_df_combined.copy()\n",
    "\n",
    "# grouping by frequency\n",
    "for col in cols_to_encode:\n",
    "    freq = salestrain_df_freq.groupby(col).size()/len(salestrain_df_freq)\n",
    "    salestrain_df_freq.loc[:, \"{}_freq_encode\".format(col)] = salestrain_df_freq[col].map(freq)\n",
    "    \n",
    "salestrain_df_freq.drop(columns=cols_to_encode, inplace=True)\n",
    "salestrain_df_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing\n",
    "Use hash functions to translate categorical feature to numeric features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently, number of bits used to represent features is a bit random (2^x where the result is bigger than number of unique values)\n",
    "# this takes quite a bit of time to run\n",
    "cols_to_encode = ['item_name', 'item_category_name', 'shop_name']\n",
    "nr_bits = [6, 7, 15]\n",
    "salestrain_df_hash = salestrain_df_combined.copy()\n",
    "\n",
    "for bits, col in zip(nr_bits, cols_to_encode):\n",
    "    print(bits, col)\n",
    "    h = FeatureHasher(n_features=bits, input_type='string') \n",
    "    hashed_col = h.fit_transform(salestrain_df_hash[col].astype(str)).toarray()\n",
    "    salestrain_df_hash = pd.concat([salestrain_df_hash, pd.DataFrame(hashed_col)], axis = 1)\n",
    "    for i in range(bits):\n",
    "        salestrain_df_hash.rename(columns={i: col + \"_\" + str(i)}, inplace=True)\n",
    "    print(col + \" done\")\n",
    "        \n",
    "salestrain_df_hash.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target/mean encoding\n",
    "Seems to work very well in lots of applications, but can cause overfitting. Maybe use (stratified) KFold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ['item_name', 'item_category_name', 'shop_name']\n",
    "salestrain_df_mean = salestrain_df_combined.copy()\n",
    "\n",
    "# simple method, probably should do this with item_cnt_month though, should create that feature\n",
    "for col in cols_to_encode:\n",
    "    mean_enc = salestrain_df_mean.groupby(col)['item_cnt_day'].mean()\n",
    "    salestrain_df_mean.loc[:, str(col + \"_mean_encoded\")] = salestrain_df_mean[col].map(mean_enc)\n",
    "    \n",
    "salestrain_df_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfTzu7o8S7t1"
   },
   "source": [
    "\n",
    "# good resources\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "\n",
    "https://www.ritchieng.com/machinelearning-one-hot-encoding/\n",
    "\n",
    "https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4mY4s3UubFhz",
    "2JVZ-b2LbqjR",
    "uHVIBl7Xcv4E",
    "MchAi0wABchP",
    "O6TAj7fUryKg"
   ],
   "include_colab_link": true,
   "name": "predict_future_sales.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
